{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e64a7e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Imports\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler, FunctionTransformer, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "set_config(transform_output=\"pandas\")  # To preserve column names in transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07009bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING - MODIFIED DATAFRAME\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING FOR MODIFIED DATAFRAME\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80) \n",
    "print(\"FEATURE ENGINEERING - MODIFIED DATAFRAME\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c976c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_o = Path('../data/raw/steel_energy_original.csv')\n",
    "df_o = pd.read_csv(data_path_o)\n",
    "\n",
    "data_path_m = Path('../data/raw/steel_energy_modified.csv')\n",
    "df_m = pd.read_csv(data_path_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ac38883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (35740, 12)\n",
      "Columns: ['date', 'Usage_kWh', 'Lagging_Current_Reactive.Power_kVarh', 'Leading_Current_Reactive_Power_kVarh', 'CO2(tCO2)', 'Lagging_Current_Power_Factor', 'Leading_Current_Power_Factor', 'NSM', 'WeekStatus', 'Day_of_week', 'Load_Type', 'mixed_type_col']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Load Data\n",
    "# ============================================================================\n",
    "# Option 1: If loading from CSV\n",
    "# df = pd.read_csv('../data/raw/energy_data.csv')\n",
    "\n",
    "# Option 2: Use existing df_m from previous notebook\n",
    "df = df_m.copy()\n",
    "\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a803e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: DATA TYPE CONVERSION\n",
      "================================================================================\n",
      "✓ Data types converted\n",
      "  Shape: (35740, 12)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Convert Data Types\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: DATA TYPE CONVERSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_cols = [\n",
    "    'Usage_kWh',\n",
    "    'Lagging_Current_Reactive.Power_kVarh',\n",
    "    'Leading_Current_Reactive_Power_kVarh',\n",
    "    'CO2(tCO2)',\n",
    "    'Lagging_Current_Power_Factor',\n",
    "    'Leading_Current_Power_Factor',\n",
    "    'NSM'\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print(f\"✓ Data types converted\")\n",
    "print(f\"  Shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: FEATURE ENGINEERING\n",
      "================================================================================\n",
      "✓ Features engineered\n",
      "  New shape: (35740, 34)\n",
      "  New columns added: 22\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Create Engineered Features (time data to cyclical)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create engineered features for energy consumption\"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # 1. Temporal Features\n",
    "    df_eng['year'] = df_eng['date'].dt.year\n",
    "    df_eng['month'] = df_eng['date'].dt.month\n",
    "    df_eng['day'] = df_eng['date'].dt.day\n",
    "    df_eng['hour'] = df_eng['date'].dt.hour\n",
    "    df_eng['day_of_week_num'] = df_eng['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df_eng['is_weekend'] = (df_eng['day_of_week_num'] >= 5).astype(int)\n",
    "    df_eng['quarter'] = df_eng['date'].dt.quarter\n",
    "    \n",
    "    # 2. Cyclical Encoding for Temporal Features\n",
    "    df_eng['hour_sin'] = np.sin(2 * np.pi * df_eng['hour'] / 24)\n",
    "    df_eng['hour_cos'] = np.cos(2 * np.pi * df_eng['hour'] / 24)\n",
    "    df_eng['month_sin'] = np.sin(2 * np.pi * df_eng['month'] / 12)\n",
    "    df_eng['month_cos'] = np.cos(2 * np.pi * df_eng['month'] / 12)\n",
    "    df_eng['dow_sin'] = np.sin(2 * np.pi * df_eng['day_of_week_num'] / 7)\n",
    "    df_eng['dow_cos'] = np.cos(2 * np.pi * df_eng['day_of_week_num'] / 7)\n",
    "    \n",
    "    # 3. Power Factor Features\n",
    "    if 'Lagging_Current_Power_Factor' in df_eng.columns and 'Leading_Current_Power_Factor' in df_eng.columns:\n",
    "        df_eng['power_factor_ratio'] = df_eng['Lagging_Current_Power_Factor'] / (df_eng['Leading_Current_Power_Factor'] + 1e-6)\n",
    "        df_eng['power_factor_diff'] = df_eng['Lagging_Current_Power_Factor'] - df_eng['Leading_Current_Power_Factor']\n",
    "        df_eng['avg_power_factor'] = (df_eng['Lagging_Current_Power_Factor'] + df_eng['Leading_Current_Power_Factor']) / 2\n",
    "    \n",
    "    # 4. Reactive Power Features\n",
    "    if 'Lagging_Current_Reactive.Power_kVarh' in df_eng.columns and 'Leading_Current_Reactive_Power_kVarh' in df_eng.columns:\n",
    "        df_eng['reactive_power_total'] = df_eng['Lagging_Current_Reactive.Power_kVarh'] + df_eng['Leading_Current_Reactive_Power_kVarh']\n",
    "        df_eng['reactive_power_diff'] = df_eng['Lagging_Current_Reactive.Power_kVarh'] - df_eng['Leading_Current_Reactive_Power_kVarh']\n",
    "        df_eng['reactive_power_ratio'] = df_eng['Lagging_Current_Reactive.Power_kVarh'] / (df_eng['Leading_Current_Reactive_Power_kVarh'] + 1e-6)\n",
    "    \n",
    "    # 5. Energy Efficiency Indicators\n",
    "    if 'CO2(tCO2)' in df_eng.columns and 'Usage_kWh' in df_eng.columns:\n",
    "        df_eng['co2_per_kwh'] = df_eng['CO2(tCO2)'] / (df_eng['Usage_kWh'] + 1e-6)\n",
    "    \n",
    "    # 6. High Consumption Flag\n",
    "    if 'Usage_kWh' in df_eng.columns:\n",
    "        df_eng['is_high_consumption'] = (df_eng['Usage_kWh'] > df_eng['Usage_kWh'].median()).astype(int)\n",
    "    \n",
    "    # 7. NSM ratio (normalized)\n",
    "    if 'NSM' in df_eng.columns and 'Usage_kWh' in df_eng.columns:\n",
    "        df_eng['nsm_per_kwh'] = df_eng['NSM'] / (df_eng['Usage_kWh'] + 1e-6)\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "df_engineered = engineer_features(df)\n",
    "print(f\"✓ Features engineered\")\n",
    "print(f\"  New shape: {df_engineered.shape}\")\n",
    "print(f\"  New columns added: {df_engineered.shape[1] - df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "064b9054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: SPLIT FEATURES AND TARGET\n",
      "================================================================================\n",
      "✓ Data split completed\n",
      "  Features shape: (35740, 31)\n",
      "  Target shape: (35740,)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Split Features and Target\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: SPLIT FEATURES AND TARGET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define columns to exclude from features\n",
    "exclude_cols = ['date', 'Usage_kWh', 'Day_of_week']\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [col for col in df_engineered.columns if col not in exclude_cols]\n",
    "\n",
    "# Create X and y\n",
    "X = df_engineered[feature_cols].copy()\n",
    "y = df_engineered['Usage_kWh'].copy()\n",
    "\n",
    "# Handle any remaining NaN values in target\n",
    "y = y.fillna(y.median())\n",
    "\n",
    "print(f\"✓ Data split completed\")\n",
    "print(f\"  Features shape: {X.shape}\")\n",
    "print(f\"  Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4aea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: DEFINE PREPROCESSING COLUMN GROUPS\n",
      "================================================================================\n",
      "Column groups defined:\n",
      "  Skewed numeric (7): ['Lagging_Current_Reactive.Power_kVarh', 'Leading_Current_Reactive_Power_kVarh', 'CO2(tCO2)', 'reactive_power_total', 'NSM', 'co2_per_kwh', 'nsm_per_kwh']\n",
      "  Linear numeric (21): ['Lagging_Current_Power_Factor', 'Leading_Current_Power_Factor', 'hour', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos', 'power_factor_ratio', 'power_factor_diff', 'avg_power_factor', 'reactive_power_diff', 'reactive_power_ratio', 'year', 'month', 'day', 'quarter', 'day_of_week_num', 'is_weekend', 'is_high_consumption']\n",
      "  Nominal categorical (1): ['WeekStatus']\n",
      "  Ordinal categorical (1): ['Load_Type']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Define Column Groups for Preprocessing \n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: DEFINE PREPROCESSING COLUMN GROUPS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Skewed numeric columns (will apply log1p transformation)\n",
    "num_skew = [\n",
    "    'Lagging_Current_Reactive.Power_kVarh',\n",
    "    'Leading_Current_Reactive_Power_kVarh',\n",
    "    'CO2(tCO2)',\n",
    "    'reactive_power_total',\n",
    "    'NSM',\n",
    "    'co2_per_kwh',\n",
    "    'nsm_per_kwh'\n",
    "]\n",
    "# Filter only existing columns\n",
    "num_skew = [col for col in num_skew if col in X.columns]\n",
    "\n",
    "# Linear numeric columns (no transformation needed)\n",
    "num_lin = [\n",
    "    'Lagging_Current_Power_Factor',\n",
    "    'Leading_Current_Power_Factor',\n",
    "    'hour', 'hour_sin', 'hour_cos',\n",
    "    'dow_sin', 'dow_cos',\n",
    "    'month_sin', 'month_cos',\n",
    "    'power_factor_ratio', 'power_factor_diff', 'avg_power_factor',\n",
    "    'reactive_power_diff', 'reactive_power_ratio',\n",
    "    'year', 'month', 'day', 'quarter',\n",
    "    'day_of_week_num', 'is_weekend', 'is_high_consumption'\n",
    "]\n",
    "# Filter only existing columns\n",
    "num_lin = [col for col in num_lin if col in X.columns]\n",
    "\n",
    "# Nominal categorical (binary/unordered)\n",
    "cat_nom = ['WeekStatus']\n",
    "cat_nom = [col for col in cat_nom if col in X.columns]\n",
    "\n",
    "# Ordinal categorical (ordered categories)\n",
    "cat_ord = ['Load_Type']\n",
    "cat_ord = [col for col in cat_ord if col in X.columns]\n",
    "\n",
    "print(f\"Column groups defined:\")\n",
    "print(f\"  Skewed numeric ({len(num_skew)}): {num_skew}\")\n",
    "print(f\"  Linear numeric ({len(num_lin)}): {num_lin}\")\n",
    "print(f\"  Nominal categorical ({len(cat_nom)}): {cat_nom}\")\n",
    "print(f\"  Ordinal categorical ({len(cat_ord)}): {cat_ord}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5dc9ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: CREATE PREPROCESSING PIPELINES\n",
      "================================================================================\n",
      "✓ Preprocessing pipelines created\n",
      "  - Skewed numeric: Imputer → Log1p → MinMaxScaler\n",
      "  - Linear numeric: Imputer → MinMaxScaler\n",
      "  - Nominal categorical: Imputer → OneHotEncoder\n",
      "  - Ordinal categorical: Imputer → OrdinalEncoder\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Create Preprocessing Pipelines\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: CREATE PREPROCESSING PIPELINES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pipeline for skewed numeric features (log1p + MinMaxScaler)\n",
    "num_skew_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('log1p', FunctionTransformer(np.log1p, feature_names_out='one-to-one')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for linear numeric features (MinMaxScaler only)\n",
    "num_lin_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for nominal categorical features (OneHotEncoder)\n",
    "nom_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first'))\n",
    "])\n",
    "\n",
    "# Pipeline for ordinal categorical features (OrdinalEncoder)\n",
    "ord_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ord', OrdinalEncoder(categories=[['Light_Load', 'Medium_Load', 'Maximum_Load']], \n",
    "                          handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "# Combine all pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num_skew', num_skew_pipe, num_skew),\n",
    "    ('num_lin',  num_lin_pipe,  num_lin),\n",
    "    ('cat_nom',  nom_pipe,      cat_nom),\n",
    "    ('cat_ord',  ord_pipe,      cat_ord),\n",
    "], remainder='drop')\n",
    "\n",
    "print(\"✓ Preprocessing pipelines created\")\n",
    "print(f\"  - Skewed numeric: Imputer → Log1p → MinMaxScaler\")\n",
    "print(f\"  - Linear numeric: Imputer → MinMaxScaler\")\n",
    "print(f\"  - Nominal categorical: Imputer → OneHotEncoder\")\n",
    "print(f\"  - Ordinal categorical: Imputer → OrdinalEncoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d1c43ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: FIT AND TRANSFORM DATA\n",
      "================================================================================\n",
      "✓ Preprocessing completed\n",
      "  Original shape: (35740, 31)\n",
      "  Processed shape: (35740, 33)\n",
      "\n",
      "Processed feature names:\n",
      "  ['num_skew__Lagging_Current_Reactive.Power_kVarh', 'num_skew__Leading_Current_Reactive_Power_kVarh', 'num_skew__CO2(tCO2)', 'num_skew__reactive_power_total', 'num_skew__NSM', 'num_skew__co2_per_kwh', 'num_skew__nsm_per_kwh', 'num_lin__Lagging_Current_Power_Factor', 'num_lin__Leading_Current_Power_Factor', 'num_lin__hour', 'num_lin__hour_sin', 'num_lin__hour_cos', 'num_lin__dow_sin', 'num_lin__dow_cos', 'num_lin__month_sin', 'num_lin__month_cos', 'num_lin__power_factor_ratio', 'num_lin__power_factor_diff', 'num_lin__avg_power_factor', 'num_lin__reactive_power_diff', 'num_lin__reactive_power_ratio', 'num_lin__year', 'num_lin__month', 'num_lin__day', 'num_lin__quarter', 'num_lin__day_of_week_num', 'num_lin__is_weekend', 'num_lin__is_high_consumption', 'cat_nom__WeekStatus_ wEEKDAY ', 'cat_nom__WeekStatus_ wEEKEND ', 'cat_nom__WeekStatus_Weekday', 'cat_nom__WeekStatus_Weekend', 'cat_ord__Load_Type']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Fit and Transform Data\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: FIT AND TRANSFORM DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Fit the preprocessor and transform the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(f\"✓ Preprocessing completed\")\n",
    "print(f\"  Original shape: {X.shape}\")\n",
    "print(f\"  Processed shape: {X_processed.shape}\")\n",
    "print(f\"\\nProcessed feature names:\")\n",
    "print(f\"  {list(X_processed.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ac4712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed data saved!\n",
      "\n",
      "Saved files:\n",
      "  • X_features.csv         (35740 rows × 33 columns)\n",
      "  • y_target.csv           (35740 rows)\n",
      "  • preprocessor.pkl       (Complete preprocessing pipeline)\n",
      "  • feature_info.pkl       (Feature metadata)\n",
      "\n",
      "================================================================================\n",
      "PIPELINE READY FOR MODEL TRAINING!\n",
      "================================================================================\n",
      "\n",
      "Example usage for modeling:\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Load preprocessor\n",
      "preprocessor = joblib.load('../data/processed/preprocessor.pkl')\n",
      "\n",
      "# Create model pipeline\n",
      "pipe_lr = Pipeline(steps=[\n",
      "    ('preprocessor', preprocessor),\n",
      "    ('model', LinearRegression())\n",
      "])\n",
      "\n",
      "# Train-test split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Fit model\n",
      "pipe_lr.fit(X_train, y_train)\n",
      "\n",
      "# Predict\n",
      "y_pred = pipe_lr.predict(X_test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Save Processed Data and Pipeline\n",
    "# ============================================================================\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save processed features and target\n",
    "X_processed.to_csv(output_dir / 'X_features.csv', index=False)\n",
    "y.to_csv(output_dir / 'y_target.csv', index=False)\n",
    "\n",
    "# Save preprocessor pipeline\n",
    "joblib.dump(preprocessor, output_dir / 'preprocessor.pkl')\n",
    "\n",
    "# Save feature information\n",
    "feature_info = {\n",
    "    'original_features': list(X.columns),\n",
    "    'processed_features': list(X_processed.columns),\n",
    "    'num_skew': num_skew,\n",
    "    'num_lin': num_lin,\n",
    "    'cat_nom': cat_nom,\n",
    "    'cat_ord': cat_ord\n",
    "}\n",
    "joblib.dump(feature_info, output_dir / 'feature_info.pkl')\n",
    "\n",
    "print(\"✅ Processed data saved!\")\n",
    "print(f\"\\nSaved files:\")\n",
    "print(f\"  • X_features.csv         ({X_processed.shape[0]} rows × {X_processed.shape[1]} columns)\")\n",
    "print(f\"  • y_target.csv           ({y.shape[0]} rows)\")\n",
    "print(f\"  • preprocessor.pkl       (Complete preprocessing pipeline)\")\n",
    "print(f\"  • feature_info.pkl       (Feature metadata)\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE READY FOR MODEL TRAINING!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nExample usage for modeling:\")\n",
    "print(\"\"\"\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load preprocessor\n",
    "preprocessor = joblib.load('../data/processed/preprocessor.pkl')\n",
    "\n",
    "# Create model pipeline\n",
    "pipe_lr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
