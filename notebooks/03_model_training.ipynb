{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e7ffad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Imports\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from imodels import RuleFitRegressor  # pip install imodels\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ddf863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: file:../mlruns\n",
      "MLflow experiment: <Experiment: artifact_location='file:c:/Users/AABDC5/repo/PythonML/dev/mlops-homework/notebooks/../mlruns/135180177654306881', creation_time=1760065591810, experiment_id='135180177654306881', last_update_time=1760065591810, lifecycle_stage='active', name='energy-consumption-forecasting', tags={}>\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Setup MLflow\n",
    "# ============================================================================\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
    "mlflow.set_experiment(\"energy-consumption-forecasting\")\n",
    "\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"MLflow experiment: {mlflow.get_experiment_by_name('energy-consumption-forecasting')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e480685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data loaded successfully\n",
      "  Features shape: (35740, 33)\n",
      "  Target shape: (35740,)\n",
      "  Feature names: ['num_skew__Lagging_Current_Reactive.Power_kVarh', 'num_skew__Leading_Current_Reactive_Power_kVarh', 'num_skew__CO2(tCO2)', 'num_skew__reactive_power_total', 'num_skew__NSM']... (33 total)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Load Processed Data\n",
    "# ============================================================================\n",
    "X = pd.read_csv('../data/processed/X_features.csv')\n",
    "y = pd.read_csv('../data/processed/y_target.csv').values.ravel()\n",
    "\n",
    "print(f\"‚úì Data loaded successfully\")\n",
    "print(f\"  Features shape: {X.shape}\")\n",
    "print(f\"  Target shape: {y.shape}\")\n",
    "print(f\"  Feature names: {list(X.columns[:5])}... ({len(X.columns)} total)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "997a516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Train-test split completed (80/20)\n",
      "  Train set: (28592, 33) ‚Üê Used for learning patterns\n",
      "  Test set:  (7148, 33)  ‚Üê Held-out for unbiased evaluation\n",
      "  Split ensures: same test data for all 8 models = fair comparison\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Train-Test Split\n",
    "# ============================================================================\n",
    "# SPLIT STRATEGY JUSTIFICATION:\n",
    "# ==============================\n",
    "# test_size=0.2 (80/20 split):\n",
    "#   - Industry standard for datasets with 5000+ samples\n",
    "#   - Provides enough test data (1600 samples) for reliable evaluation\n",
    "#   - Maintains sufficient training data (6400 samples) for learning\n",
    "# \n",
    "# random_state=42:\n",
    "#   - Ensures reproducible splits across experiments\n",
    "#   - Critical for MLOps: same test set = fair model comparison\n",
    "#   - The number 42 is arbitrary but conventional (Hitchhiker's Guide reference)\n",
    "#\n",
    "# No stratification:\n",
    "#   - Regression task (stratify only applies to classification)\n",
    "#   - Random split adequate for time-series-like energy data\n",
    "#   - Consider TimeSeriesSplit for production if temporal ordering matters\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Train-test split completed (80/20)\")\n",
    "print(f\"  Train set: {X_train.shape} ‚Üê Used for learning patterns\")\n",
    "print(f\"  Test set:  {X_test.shape}  ‚Üê Held-out for unbiased evaluation\")\n",
    "print(f\"  Split ensures: same test data for all 8 models = fair comparison\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c831f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Helper Function - Evaluate Model\n",
    "# ============================================================================\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Calculate regression metrics with business interpretation\n",
    "    \n",
    "    METRIC SELECTION RATIONALE:\n",
    "    ===========================\n",
    "    1. MSE (Mean Squared Error):\n",
    "       - Penalizes large errors heavily (squared term)\n",
    "       - Useful when large deviations are costly (e.g., capacity planning)\n",
    "       - Units: (kWh)¬≤ - harder to interpret\n",
    "    \n",
    "    2. RMSE (Root Mean Squared Error):\n",
    "       - Same units as target (kWh) - easier interpretation\n",
    "       - Shows typical prediction error magnitude\n",
    "       - Business use: \"Model predicts within ¬±X kWh on average\"\n",
    "    \n",
    "    3. MAE (Mean Absolute Error):\n",
    "       - Average absolute deviation in kWh\n",
    "       - More robust to outliers than RMSE\n",
    "       - Business use: \"Average prediction error is X kWh\"\n",
    "    \n",
    "    4. R¬≤ (R-squared / Coefficient of Determination):\n",
    "       - Percentage of variance explained (0-1 scale)\n",
    "       - 0.8 = model explains 80% of consumption variability\n",
    "       - Business use: Model quality indicator for stakeholders\n",
    "    \n",
    "    5. MAPE (Mean Absolute Percentage Error):\n",
    "       - Error as percentage of actual value\n",
    "       - Scale-independent, easy to communicate\n",
    "       - Business use: \"Model is off by X% on average\"\n",
    "       - Preferred by business analysts for forecasting accuracy\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'mse': mean_squared_error(y_true, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'r2_score': r2_score(y_true, y_pred),\n",
    "        'mape': np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100  # Add small epsilon to avoid division by zero\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  MSE:  {metrics['mse']:.4f}  ‚Üê Penalizes large errors\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.4f} kWh  ‚Üê Typical prediction error\")\n",
    "    print(f\"  MAE:  {metrics['mae']:.4f} kWh  ‚Üê Average absolute error\")\n",
    "    print(f\"  R¬≤:   {metrics['r2_score']:.4f}  ‚Üê Variance explained ({metrics['r2_score']*100:.1f}%)\")\n",
    "    print(f\"  MAPE: {metrics['mape']:.2f}%  ‚Üê Average % error (business-friendly)\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21b2db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 1: LINEAR REGRESSION\n",
      "================================================================================\n",
      "üìã Parameter Justification:\n",
      "  ‚Ä¢ fit_intercept=True: Captures baseline energy consumption\n",
      "  ‚Ä¢ normalize=False: Features pre-scaled in preprocessing phase\n",
      "\n",
      "Linear Regression Results:\n",
      "  MSE:  37451.1270  ‚Üê Penalizes large errors\n",
      "  RMSE: 193.5229 kWh  ‚Üê Typical prediction error\n",
      "  MAE:  55.0160 kWh  ‚Üê Average absolute error\n",
      "  R¬≤:   0.1904  ‚Üê Variance explained (19.0%)\n",
      "  MAPE: 714.12%  ‚Üê Average % error (business-friendly)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/10 19:59:17 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/10 19:59:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Model 1 - Linear Regression\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 1: LINEAR REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with mlflow.start_run(run_name=\"linear_regression_v1\"):\n",
    "    \n",
    "    # Log parameters with justification\n",
    "    params = {\n",
    "        'model_type': 'LinearRegression',\n",
    "        'fit_intercept': True,   # REASON: Allow baseline consumption (y-intercept) \n",
    "                                  # when all features are zero - represents base load\n",
    "        'normalize': False        # REASON: Data already scaled in feature engineering step\n",
    "                                  # (StandardScaler applied), no need for additional normalization\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    print(\"üìã Parameter Justification:\")\n",
    "    print(\"  ‚Ä¢ fit_intercept=True: Captures baseline energy consumption\")\n",
    "    print(\"  ‚Ä¢ normalize=False: Features pre-scaled in preprocessing phase\")\n",
    "    \n",
    "    # Train model\n",
    "    model_lr = LinearRegression()\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model_lr.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(y_test, y_pred, \"Linear Regression\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model_lr, \"model\")\n",
    "    \n",
    "    # Log feature names\n",
    "    mlflow.log_dict({'features': list(X.columns)}, 'features.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0228e5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 2: K-NEAREST NEIGHBORS\n",
      "================================================================================\n",
      "üìã Parameter Justification:\n",
      "  ‚Ä¢ n_neighbors=5: Balances local pattern detection vs generalization\n",
      "  ‚Ä¢ weights=uniform: Equal importance to all 5 nearest neighbors\n",
      "  ‚Ä¢ algorithm=auto: Automatically selects most efficient search algorithm\n",
      "  ‚Ä¢ metric=minkowski: Standard distance measure for scaled numerical data\n",
      "\n",
      "K-Nearest Neighbors Results:\n",
      "  MSE:  24177.1116  ‚Üê Penalizes large errors\n",
      "  RMSE: 155.4899 kWh  ‚Üê Typical prediction error\n",
      "  MAE:  13.9523 kWh  ‚Üê Average absolute error\n",
      "  R¬≤:   0.4773  ‚Üê Variance explained (47.7%)\n",
      "  MAPE: 31.24%  ‚Üê Average % error (business-friendly)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/10 19:59:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/10 20:00:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Model 2 - K-Nearest Neighbors (KNN)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 2: K-NEAREST NEIGHBORS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with mlflow.start_run(run_name=\"knn_v1\"):\n",
    "    \n",
    "    # Parameters with justification\n",
    "    params = {\n",
    "        'model_type': 'KNeighborsRegressor',\n",
    "        'n_neighbors': 5,      # REASON: Square root rule (‚àön ‚âà 90 for 8000 samples)\n",
    "                               # Balances bias-variance: too few=overfitting, too many=underfitting\n",
    "                               # 5 is a good starting point for most datasets\n",
    "        'weights': 'uniform',  # REASON: Equal weight to all neighbors (baseline approach)\n",
    "                               # Alternative 'distance' gives more weight to closer neighbors\n",
    "        'algorithm': 'auto',   # REASON: Let sklearn choose best algorithm (ball_tree/kd_tree/brute)\n",
    "                               # based on data structure - optimal for performance\n",
    "        'metric': 'minkowski'  # REASON: Generalized distance metric (p=2 gives Euclidean)\n",
    "                               # Works well with scaled features\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    print(\"üìã Parameter Justification:\")\n",
    "    print(\"  ‚Ä¢ n_neighbors=5: Balances local pattern detection vs generalization\")\n",
    "    print(\"  ‚Ä¢ weights=uniform: Equal importance to all 5 nearest neighbors\")\n",
    "    print(\"  ‚Ä¢ algorithm=auto: Automatically selects most efficient search algorithm\")\n",
    "    print(\"  ‚Ä¢ metric=minkowski: Standard distance measure for scaled numerical data\")\n",
    "    \n",
    "    # Train model\n",
    "    model_params = {k: v for k, v in params.items() if k != 'model_type'}\n",
    "    model_knn = KNeighborsRegressor(**model_params)\n",
    "    model_knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model_knn.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(y_test, y_pred, \"K-Nearest Neighbors\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model_knn, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1c5b86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 3: RANDOM FOREST\n",
      "================================================================================\n",
      "üìã Parameter Justification:\n",
      "  ‚Ä¢ n_estimators=100: Standard ensemble size balancing accuracy & speed\n",
      "  ‚Ä¢ max_depth=20: Deep enough for complex patterns, shallow enough to avoid overfitting\n",
      "  ‚Ä¢ min_samples_split=5: Prevents splitting on noise, improves generalization\n",
      "  ‚Ä¢ min_samples_leaf=2: Ensures robust predictions from multiple samples\n",
      "  ‚Ä¢ max_features='sqrt': Decorrelates trees for better ensemble diversity\n",
      "  ‚Ä¢ n_jobs=-1: Parallel processing for faster training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/10 20:00:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results:\n",
      "  MSE:  8714.8328  ‚Üê Penalizes large errors\n",
      "  RMSE: 93.3533 kWh  ‚Üê Typical prediction error\n",
      "  MAE:  9.4844 kWh  ‚Üê Average absolute error\n",
      "  R¬≤:   0.8116  ‚Üê Variance explained (81.2%)\n",
      "  MAPE: 39.35%  ‚Üê Average % error (business-friendly)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/10 20:00:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Important Features:\n",
      "  num_skew__nsm_per_kwh: 0.3269\n",
      "  num_skew__co2_per_kwh: 0.1664\n",
      "  num_lin__reactive_power_diff: 0.0586\n",
      "  num_lin__Lagging_Current_Power_Factor: 0.0547\n",
      "  num_skew__NSM: 0.0488\n",
      "  num_lin__reactive_power_ratio: 0.0440\n",
      "  num_skew__Lagging_Current_Reactive.Power_kVarh: 0.0395\n",
      "  num_lin__power_factor_ratio: 0.0370\n",
      "  num_lin__avg_power_factor: 0.0363\n",
      "  num_skew__reactive_power_total: 0.0348\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Model 3 - Random Forest\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 3: RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with mlflow.start_run(run_name=\"random_forest_v1\"):\n",
    "    \n",
    "    # Parameters with justification\n",
    "    params = {\n",
    "        'model_type': 'RandomForestRegressor',\n",
    "        'n_estimators': 100,      # REASON: 100 trees balances performance vs computation time\n",
    "                                   # More trees = better performance but diminishing returns after 100\n",
    "                                   # Industry standard for initial experimentation\n",
    "        'max_depth': 20,          # REASON: Limits tree depth to prevent overfitting\n",
    "                                   # 20 allows complex patterns without memorizing training data\n",
    "                                   # Energy data has ~50 features, depth=20 captures interactions\n",
    "        'min_samples_split': 5,   # REASON: Minimum 5 samples required to split a node\n",
    "                                   # Prevents creating splits on noise/outliers\n",
    "                                   # Higher value = more generalization, less overfitting\n",
    "        'min_samples_leaf': 2,    # REASON: At least 2 samples in leaf nodes\n",
    "                                   # Ensures predictions based on multiple observations\n",
    "                                   # Reduces variance in leaf predictions\n",
    "        'max_features': 'sqrt',   # REASON: Consider ‚àö(n_features) for each split\n",
    "                                   # Adds randomness to decorrelate trees\n",
    "                                   # Standard practice for regression (classification uses 'log2')\n",
    "        'random_state': 42,       # REASON: Reproducibility - ensures same results across runs\n",
    "                                   # Critical for MLOps pipeline consistency\n",
    "        'n_jobs': -1              # REASON: Use all CPU cores for parallel training\n",
    "                                   # Significantly speeds up training time\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    print(\"üìã Parameter Justification:\")\n",
    "    print(\"  ‚Ä¢ n_estimators=100: Standard ensemble size balancing accuracy & speed\")\n",
    "    print(\"  ‚Ä¢ max_depth=20: Deep enough for complex patterns, shallow enough to avoid overfitting\")\n",
    "    print(\"  ‚Ä¢ min_samples_split=5: Prevents splitting on noise, improves generalization\")\n",
    "    print(\"  ‚Ä¢ min_samples_leaf=2: Ensures robust predictions from multiple samples\")\n",
    "    print(\"  ‚Ä¢ max_features='sqrt': Decorrelates trees for better ensemble diversity\")\n",
    "    print(\"  ‚Ä¢ n_jobs=-1: Parallel processing for faster training\")\n",
    "    \n",
    "    # Train model\n",
    "    model_params = {k: v for k, v in params.items() if k != 'model_type'}\n",
    "    model_rf = RandomForestRegressor(**model_params)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model_rf.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(y_test, y_pred, \"Random Forest\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model_rf, \"model\")\n",
    "    \n",
    "    # Log feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': model_rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    mlflow.log_dict(feature_importance.to_dict(), 'feature_importance.json')\n",
    "    \n",
    "    print(\"\\nTop 10 Important Features:\")\n",
    "    for idx, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "411a847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 4: XGBOOST\n",
      "================================================================================\n",
      "üìã Parameter Justification:\n",
      "  ‚Ä¢ n_estimators=100: Sufficient boosting rounds for convergence\n",
      "  ‚Ä¢ learning_rate=0.1: Conservative step size prevents overfitting\n",
      "  ‚Ä¢ max_depth=6: Optimal depth for boosted trees (shallower than RF)\n",
      "  ‚Ä¢ subsample=0.8: Row sampling adds randomness, reduces overfitting\n",
      "  ‚Ä¢ colsample_bytree=0.8: Column sampling decorrelates trees\n",
      "  ‚Ä¢ reg_lambda=1: L2 regularization for weight control & generalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/10 20:01:01 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Results:\n",
      "  MSE:  13937.5805  ‚Üê Penalizes large errors\n",
      "  RMSE: 118.0575 kWh  ‚Üê Typical prediction error\n",
      "  MAE:  7.1857 kWh  ‚Üê Average absolute error\n",
      "  R¬≤:   0.6987  ‚Üê Variance explained (69.9%)\n",
      "  MAPE: 20.17%  ‚Üê Average % error (business-friendly)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/10 20:01:17 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Important Features:\n",
      "  num_lin__is_high_consumption: 0.1420\n",
      "  num_skew__nsm_per_kwh: 0.1412\n",
      "  cat_ord__Load_Type: 0.1087\n",
      "  num_skew__NSM: 0.0779\n",
      "  num_skew__Lagging_Current_Reactive.Power_kVarh: 0.0768\n",
      "  num_skew__co2_per_kwh: 0.0676\n",
      "  num_lin__power_factor_ratio: 0.0616\n",
      "  num_lin__reactive_power_diff: 0.0601\n",
      "  num_lin__Lagging_Current_Power_Factor: 0.0599\n",
      "  num_skew__reactive_power_total: 0.0545\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Model 4 - XGBoost\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 4: XGBOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with mlflow.start_run(run_name=\"xgboost_v1\"):\n",
    "    \n",
    "    # Parameters with justification\n",
    "    params = {\n",
    "        'model_type': 'XGBRegressor',\n",
    "        'n_estimators': 100,       # REASON: 100 boosting rounds - standard starting point\n",
    "                                    # XGBoost converges faster than RF, 100 is often sufficient\n",
    "                                    # Can be tuned with early_stopping in production\n",
    "        'learning_rate': 0.1,      # REASON: Controls step size at each boosting iteration\n",
    "                                    # 0.1 is conservative, prevents overfitting\n",
    "                                    # Lower = more robust but needs more estimators\n",
    "                                    # Higher = faster training but risk of overfitting\n",
    "        'max_depth': 6,            # REASON: Shallower than RF (6 vs 20) because boosting builds sequentially\n",
    "                                    # Depth 6 = up to 64 leaf nodes, sufficient for most patterns\n",
    "                                    # XGBoost default, proven effective across domains\n",
    "        'min_child_weight': 1,     # REASON: Minimum sum of instance weight in child node\n",
    "                                    # Controls overfitting similar to min_samples_leaf\n",
    "                                    # 1 = allow smaller partitions for detailed patterns\n",
    "        'subsample': 0.8,          # REASON: Use 80% of samples for each tree (row sampling)\n",
    "                                    # Prevents overfitting by introducing randomness\n",
    "                                    # 0.8 balances variance reduction with training stability\n",
    "        'colsample_bytree': 0.8,   # REASON: Use 80% of features for each tree (column sampling)\n",
    "                                    # Similar to RF's max_features, adds diversity\n",
    "                                    # Reduces correlation between trees\n",
    "        'gamma': 0,                # REASON: Minimum loss reduction to make split (0 = no constraint)\n",
    "                                    # 0 for initial model, can increase for regularization\n",
    "                                    # Acts as pruning parameter\n",
    "        'reg_alpha': 0,            # REASON: L1 regularization (Lasso) - not applied initially\n",
    "                                    # 0 = no feature selection penalty\n",
    "                                    # Can be tuned if many irrelevant features suspected\n",
    "        'reg_lambda': 1,           # REASON: L2 regularization (Ridge) - mild regularization\n",
    "                                    # 1 = default, prevents extreme weights\n",
    "                                    # Helps with generalization without being too restrictive\n",
    "        'random_state': 42,        # REASON: Reproducibility for MLOps pipeline\n",
    "        'n_jobs': -1               # REASON: Parallel processing for faster training\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    print(\"üìã Parameter Justification:\")\n",
    "    print(\"  ‚Ä¢ n_estimators=100: Sufficient boosting rounds for convergence\")\n",
    "    print(\"  ‚Ä¢ learning_rate=0.1: Conservative step size prevents overfitting\")\n",
    "    print(\"  ‚Ä¢ max_depth=6: Optimal depth for boosted trees (shallower than RF)\")\n",
    "    print(\"  ‚Ä¢ subsample=0.8: Row sampling adds randomness, reduces overfitting\")\n",
    "    print(\"  ‚Ä¢ colsample_bytree=0.8: Column sampling decorrelates trees\")\n",
    "    print(\"  ‚Ä¢ reg_lambda=1: L2 regularization for weight control & generalization\")\n",
    "    \n",
    "    # Train model\n",
    "    model_params = {k: v for k, v in params.items() if k != 'model_type'}\n",
    "    model_xgb = xgb.XGBRegressor(**model_params)\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model_xgb.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(y_test, y_pred, \"XGBoost\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model_xgb, \"model\")\n",
    "    \n",
    "    # Log feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': model_xgb.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    mlflow.log_dict(feature_importance.to_dict(), 'feature_importance.json')\n",
    "    \n",
    "    print(\"\\nTop 10 Important Features:\")\n",
    "    for idx, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be132908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 5: RULEFIT (RULE-BASED REGRESSION)\n",
      "================================================================================\n",
      "üìã Parameter Justification:\n",
      "  ‚Ä¢ max_rules=30: Balances coverage with interpretability for stakeholders\n",
      "  ‚Ä¢ tree_size=4: Generates understandable rules (max 4 conditions per rule)\n",
      "  ‚Ä¢ exp_rand_tree_size=True: Creates diverse rule complexity for better coverage\n",
      "  ‚Ä¢ Business Value: Rules can be validated by domain experts and implemented in systems\n",
      "\n",
      "RuleFit Results:\n",
      "  MSE:  3189.9299  ‚Üê Penalizes large errors\n",
      "  RMSE: 56.4795 kWh  ‚Üê Typical prediction error\n",
      "  MAE:  8.6153 kWh  ‚Üê Average absolute error\n",
      "  R¬≤:   0.9310  ‚Üê Variance explained (93.1%)\n",
      "  MAPE: 65.25%  ‚Üê Average % error (business-friendly)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/10 21:13:42 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/10 21:14:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è  Could not extract rules: 'RuleFitRegressor' object has no attribute 'get_rules'\n",
      "  Model trained successfully, but rule extraction unavailable\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Model 5 - RuleFit (Rule-Based Regression)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 5: RULEFIT (RULE-BASED REGRESSION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with mlflow.start_run(run_name=\"rulefit_v1\"):\n",
    "    \n",
    "    # Parameters with justification\n",
    "    params = {\n",
    "        'model_type': 'RuleFitRegressor',\n",
    "        'max_rules': 30,           # REASON: Limits number of rules for interpretability\n",
    "                                    # 30 rules = manageable for human review & business validation\n",
    "                                    # Too many rules = loss of interpretability\n",
    "                                    # Too few = may miss important patterns\n",
    "        'tree_size': 4,            # REASON: Maximum tree depth for rule extraction\n",
    "                                    # Depth 4 = rules with up to 4 conditions (IF A AND B AND C AND D)\n",
    "                                    # Shallow trees = simpler, more interpretable rules\n",
    "                                    # Deeper than 6 creates overly complex rules\n",
    "        'random_state': 42,        # REASON: Reproducibility for consistent rule generation\n",
    "        'exp_rand_tree_size': True # REASON: Exponentially sample tree depths (1, 2, 4, 8...)\n",
    "                                    # Creates diverse rule complexity (simple + complex)\n",
    "                                    # Improves ensemble diversity and pattern coverage\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    print(\"üìã Parameter Justification:\")\n",
    "    print(\"  ‚Ä¢ max_rules=30: Balances coverage with interpretability for stakeholders\")\n",
    "    print(\"  ‚Ä¢ tree_size=4: Generates understandable rules (max 4 conditions per rule)\")\n",
    "    print(\"  ‚Ä¢ exp_rand_tree_size=True: Creates diverse rule complexity for better coverage\")\n",
    "    print(\"  ‚Ä¢ Business Value: Rules can be validated by domain experts and implemented in systems\")\n",
    "    \n",
    "    # Train model\n",
    "    model_params = {k: v for k, v in params.items() if k != 'model_type'}\n",
    "    model_rf = RuleFitRegressor(**model_params)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model_rf.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(y_test, y_pred, \"RuleFit\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model_rf, \"model\")\n",
    "    \n",
    "    # Extract and log rules\n",
    "    try:\n",
    "        rules = model_rf.get_rules()\n",
    "        if rules is not None and len(rules) > 0:\n",
    "            rules_df = pd.DataFrame(rules)\n",
    "            top_rules = rules_df.nlargest(10, 'importance') if 'importance' in rules_df.columns else rules_df.head(10)\n",
    "            mlflow.log_dict(top_rules.to_dict(), 'top_rules.json')\n",
    "            \n",
    "            print(f\"\\n‚úì Generated {len(rules)} rules\")\n",
    "            print(f\"\\nTop 5 Most Important Rules:\")\n",
    "            for idx, row in top_rules.head(5).iterrows():\n",
    "                if 'rule' in row and 'importance' in row:\n",
    "                    print(f\"  Rule: {row['rule']}\")\n",
    "                    print(f\"  Importance: {row['importance']:.4f}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Could not extract rules: {e}\")\n",
    "        print(\"  Model trained successfully, but rule extraction unavailable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "641cc939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 6: PCA + LINEAR REGRESSION\n",
      "================================================================================\n",
      "üìã Parameter Justification:\n",
      "  ‚Ä¢ n_components=0.95: Keeps 95% of data variance while reducing dimensions\n",
      "  ‚Ä¢ whiten=False: Features pre-scaled, whitening unnecessary\n",
      "  ‚Ä¢ Business Benefit: Addresses multicollinearity in correlated energy metrics\n",
      "\n",
      "‚úì PCA reduced features from 33 to 12\n",
      "  Explained variance: 0.9547\n",
      "\n",
      "PCA + Linear Regression Results:\n",
      "  MSE:  44514.1979  ‚Üê Penalizes large errors\n",
      "  RMSE: 210.9839 kWh  ‚Üê Typical prediction error\n",
      "  MAE:  33.0836 kWh  ‚Üê Average absolute error\n",
      "  R¬≤:   0.0377  ‚Üê Variance explained (3.8%)\n",
      "  MAPE: 307.68%  ‚Üê Average % error (business-friendly)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/10 21:14:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/10/10 21:15:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: Model 6 - PCA + Linear Regression\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 6: PCA + LINEAR REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with mlflow.start_run(run_name=\"pca_linear_regression_v1\"):\n",
    "    \n",
    "    # Parameters with justification\n",
    "    params = {\n",
    "        'model_type': 'PCA_LinearRegression',\n",
    "        'n_components': 0.95,  # REASON: Retain 95% of variance - standard in literature\n",
    "                               # Reduces dimensionality while preserving most information\n",
    "                               # Helps with multicollinearity in energy data (correlated features)\n",
    "                               # 95% balances information retention vs dimensionality reduction\n",
    "        'whiten': False        # REASON: No whitening (variance scaling) of PCA components\n",
    "                               # Features already standardized in preprocessing\n",
    "                               # Whitening can reduce interpretability of components\n",
    "                               # False = preserve relative importance of principal components\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    print(\"üìã Parameter Justification:\")\n",
    "    print(\"  ‚Ä¢ n_components=0.95: Keeps 95% of data variance while reducing dimensions\")\n",
    "    print(\"  ‚Ä¢ whiten=False: Features pre-scaled, whitening unnecessary\")\n",
    "    print(\"  ‚Ä¢ Business Benefit: Addresses multicollinearity in correlated energy metrics\")\n",
    "    \n",
    "    # Create pipeline with PCA and Linear Regression\n",
    "    pipeline = Pipeline([\n",
    "        ('pca', PCA(n_components=params['n_components'], whiten=params['whiten'])),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Train pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Get number of components selected\n",
    "    n_components_selected = pipeline.named_steps['pca'].n_components_\n",
    "    explained_variance = pipeline.named_steps['pca'].explained_variance_ratio_.sum()\n",
    "    \n",
    "    mlflow.log_param('n_components_selected', n_components_selected)\n",
    "    mlflow.log_param('explained_variance', explained_variance)\n",
    "    \n",
    "    print(f\"\\n‚úì PCA reduced features from {X.shape[1]} to {n_components_selected}\")\n",
    "    print(f\"  Explained variance: {explained_variance:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(y_test, y_pred, \"PCA + Linear Regression\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(pipeline, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8147cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: Model 7 - PCA + Random Forest\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 7: PCA + RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with mlflow.start_run(run_name=\"pca_random_forest_v1\"):\n",
    "    \n",
    "    # Parameters with justification\n",
    "    params = {\n",
    "        'model_type': 'PCA_RandomForest',\n",
    "        'n_components': 0.95,  # REASON: 95% variance retention (consistent with PCA+LR)\n",
    "                               # Tests if PCA helps tree-based models (usually less effective)\n",
    "                               # RF handles high dimensions well, but PCA can speed training\n",
    "        'n_estimators': 100,   # REASON: Same as standalone RF for fair comparison\n",
    "                               # 100 trees is sufficient even with reduced dimensions\n",
    "        'max_depth': 20,       # REASON: Same depth as standalone RF\n",
    "                               # PCA components are linear combinations, may need depth\n",
    "        'random_state': 42     # REASON: Reproducibility\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    print(\"üìã Parameter Justification:\")\n",
    "    print(\"  ‚Ä¢ n_components=0.95: Tests if dimensionality reduction helps RF performance\")\n",
    "    print(\"  ‚Ä¢ n_estimators=100: Consistent with standalone RF for comparison\")\n",
    "    print(\"  ‚Ä¢ max_depth=20: Maintains complexity despite fewer features\")\n",
    "    print(\"  ‚Ä¢ Use Case: May improve speed, unlikely to improve accuracy for RF\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('pca', PCA(n_components=params['n_components'])),\n",
    "        ('regressor', RandomForestRegressor(\n",
    "            n_estimators=params['n_estimators'],\n",
    "            max_depth=params['max_depth'],\n",
    "            random_state=params['random_state'],\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Get PCA info\n",
    "    n_components_selected = pipeline.named_steps['pca'].n_components_\n",
    "    explained_variance = pipeline.named_steps['pca'].explained_variance_ratio_.sum()\n",
    "    \n",
    "    mlflow.log_param('n_components_selected', n_components_selected)\n",
    "    mlflow.log_param('explained_variance', explained_variance)\n",
    "    \n",
    "    print(f\"\\n‚úì PCA reduced features from {X.shape[1]} to {n_components_selected}\")\n",
    "    print(f\"  Explained variance: {explained_variance:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(y_test, y_pred, \"PCA + Random Forest\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(pipeline, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "161870cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 8: PCA + XGBOOST\n",
      "================================================================================\n",
      "üìã Parameter Justification:\n",
      "  ‚Ä¢ n_components=0.95: Reduces feature space while keeping 95% information\n",
      "  ‚Ä¢ n_estimators=100: Consistent boosting rounds for fair comparison\n",
      "  ‚Ä¢ learning_rate=0.1: Conservative rate unchanged by PCA transformation\n",
      "  ‚Ä¢ max_depth=6: Standard XGBoost depth works well with PCA components\n",
      "  ‚Ä¢ Expected Benefit: Faster training, potentially better generalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/10 21:15:28 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì PCA reduced features from 33 to 12\n",
      "  Explained variance: 0.9547\n",
      "\n",
      "PCA + XGBoost Results:\n",
      "  MSE:  45859.0867  ‚Üê Penalizes large errors\n",
      "  RMSE: 214.1473 kWh  ‚Üê Typical prediction error\n",
      "  MAE:  23.8554 kWh  ‚Üê Average absolute error\n",
      "  R¬≤:   0.0086  ‚Üê Variance explained (0.9%)\n",
      "  MAPE: 89.76%  ‚Üê Average % error (business-friendly)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/10 21:15:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: Model 8 - PCA + XGBoost\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 8: PCA + XGBOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with mlflow.start_run(run_name=\"pca_xgboost_v1\"):\n",
    "    \n",
    "    # Parameters with justification\n",
    "    params = {\n",
    "        'model_type': 'PCA_XGBoost',\n",
    "        'n_components': 0.95,   # REASON: 95% variance retention (consistent across PCA models)\n",
    "                                # XGBoost handles high dimensions, but PCA may help with:\n",
    "                                # 1) Training speed (fewer features to evaluate per split)\n",
    "                                # 2) Reducing noise from low-variance components\n",
    "        'n_estimators': 100,    # REASON: Same as standalone XGBoost for fair comparison\n",
    "                                # Boosting may converge faster with uncorrelated PCA components\n",
    "        'learning_rate': 0.1,   # REASON: Same conservative rate as standalone XGBoost\n",
    "                                # PCA doesn't change optimal learning rate significantly\n",
    "        'max_depth': 6,         # REASON: XGBoost standard depth maintained\n",
    "                                # PCA components are continuous, depth 6 still appropriate\n",
    "        'random_state': 42      # REASON: Reproducibility across experiments\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    print(\"üìã Parameter Justification:\")\n",
    "    print(\"  ‚Ä¢ n_components=0.95: Reduces feature space while keeping 95% information\")\n",
    "    print(\"  ‚Ä¢ n_estimators=100: Consistent boosting rounds for fair comparison\")\n",
    "    print(\"  ‚Ä¢ learning_rate=0.1: Conservative rate unchanged by PCA transformation\")\n",
    "    print(\"  ‚Ä¢ max_depth=6: Standard XGBoost depth works well with PCA components\")\n",
    "    print(\"  ‚Ä¢ Expected Benefit: Faster training, potentially better generalization\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('pca', PCA(n_components=params['n_components'])),\n",
    "        ('regressor', xgb.XGBRegressor(\n",
    "            n_estimators=params['n_estimators'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            max_depth=params['max_depth'],\n",
    "            random_state=params['random_state'],\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Get PCA info\n",
    "    n_components_selected = pipeline.named_steps['pca'].n_components_\n",
    "    explained_variance = pipeline.named_steps['pca'].explained_variance_ratio_.sum()\n",
    "    \n",
    "    mlflow.log_param('n_components_selected', n_components_selected)\n",
    "    mlflow.log_param('explained_variance', explained_variance)\n",
    "    \n",
    "    print(f\"\\n‚úì PCA reduced features from {X.shape[1]} to {n_components_selected}\")\n",
    "    print(f\"  Explained variance: {explained_variance:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(y_test, y_pred, \"PCA + XGBoost\")\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(pipeline, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36a1f826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä Performance Comparison (sorted by RMSE):\n",
      "                   Model       RMSE       MAE        R¬≤   MAPE (%)\n",
      "              rulefit_v1  56.479465  8.615257  0.931039  65.247918\n",
      "        random_forest_v1  93.353269  9.484375  0.811600  39.352744\n",
      "              xgboost_v1 118.057531  7.185674  0.698693  20.165720\n",
      "                  knn_v1 155.489908 13.952253  0.477332  31.237594\n",
      "              xgboost_v1 178.845295 15.054670  0.308525  99.029948\n",
      "              xgboost_v1 178.845295 15.054670  0.308525  99.029948\n",
      "              xgboost_v1 178.845295 15.054670  0.308525  99.029948\n",
      "        random_forest_v1 192.748294 22.110672  0.196839 117.423930\n",
      "        random_forest_v1 192.748294 22.110672  0.196839 117.423930\n",
      "        random_forest_v1 192.748294 22.110672  0.196839 117.423930\n",
      "    linear_regression_v1 193.522937 55.016029  0.190371 714.122902\n",
      "pca_linear_regression_v1 210.983881 33.083637  0.037679 307.679102\n",
      "          pca_xgboost_v1 214.147348 23.855385  0.008605  89.762837\n",
      "    linear_regression_v1 214.537385 37.066672  0.004990 376.322417\n",
      "    linear_regression_v1 214.537385 37.066672  0.004990 376.322417\n",
      "    linear_regression_v1 214.537385 37.066672  0.004990 376.322417\n",
      "pca_linear_regression_v1 214.840252 38.087532  0.002179 423.285352\n",
      "pca_linear_regression_v1 214.840252 38.087532  0.002179 423.285352\n",
      "pca_linear_regression_v1 214.840252 38.087532  0.002179 423.285352\n",
      "          pca_xgboost_v1 259.674600 38.659209 -0.457741 279.743941\n",
      "          pca_xgboost_v1 259.674600 38.659209 -0.457741 279.743941\n",
      "    pca_random_forest_v1 279.173878 36.925323 -0.684888 277.387225\n",
      "                  knn_v1 283.208148 43.452074 -0.733935 315.504123\n",
      "                  knn_v1 283.208148 43.452074 -0.733935 315.504123\n",
      "                  knn_v1 283.208148 43.452074 -0.733935 315.504123\n",
      "    pca_random_forest_v1        NaN       NaN       NaN        NaN\n",
      "    pca_random_forest_v1        NaN       NaN       NaN        NaN\n",
      "\n",
      "================================================================================\n",
      "BEST MODEL\n",
      "================================================================================\n",
      "üèÜ Model: rulefit_v1\n",
      "   RMSE: 56.4795 kWh\n",
      "   R¬≤:   0.9310 (93.1% variance explained)\n",
      "\n",
      "================================================================================\n",
      "MODEL SELECTION INSIGHTS\n",
      "================================================================================\n",
      "\n",
      "EXPECTED PERFORMANCE PATTERNS:\n",
      "================================\n",
      "1. LINEAR REGRESSION: \n",
      "   - Baseline model, assumes linear relationships\n",
      "   - Good if energy consumption is linear function of features\n",
      "   - Expected: Moderate performance, high interpretability\n",
      "\n",
      "2. K-NEAREST NEIGHBORS:\n",
      "   - Captures local patterns and non-linearity\n",
      "   - Sensitive to feature scaling (already handled)\n",
      "   - Expected: Good for similar time periods/conditions\n",
      "\n",
      "3. RANDOM FOREST:\n",
      "   - Handles non-linear relationships and interactions\n",
      "   - Robust to outliers, provides feature importance\n",
      "   - Expected: Strong performance, slower predictions\n",
      "\n",
      "4. XGBOOST:\n",
      "   - Often best performer for tabular data\n",
      "   - Handles complex patterns via sequential boosting\n",
      "   - Expected: Top 2 model, efficient predictions\n",
      "\n",
      "5. RULEFIT:\n",
      "   - Generates interpretable rules for stakeholders\n",
      "   - Balances accuracy with explainability\n",
      "   - Expected: Good performance + business insights\n",
      "\n",
      "6. PCA + LINEAR REGRESSION:\n",
      "   - Addresses multicollinearity issues\n",
      "   - Reduces overfitting via dimensionality reduction\n",
      "   - Expected: Better than plain LR if features correlated\n",
      "\n",
      "7. PCA + RANDOM FOREST:\n",
      "   - Tests if PCA helps tree-based models\n",
      "   - May speed up training, unlikely to improve accuracy\n",
      "   - Expected: Similar to RF, faster training\n",
      "\n",
      "8. PCA + XGBOOST:\n",
      "   - Combines dimensionality reduction with boosting\n",
      "   - May improve generalization\n",
      "   - Expected: Competitive with pure XGBoost\n",
      "\n",
      "SELECTION CRITERIA:\n",
      "===================\n",
      "‚Ä¢ PRODUCTION (Speed + Accuracy): Choose XGBoost or RF\n",
      "‚Ä¢ INTERPRETABILITY (Business): Choose RuleFit or Linear Regression  \n",
      "‚Ä¢ DEPLOYMENT (Low latency): Choose Linear Regression or KNN\n",
      "‚Ä¢ RESEARCH (Best RMSE): Choose top performer regardless of complexity\n",
      "\n",
      "\n",
      "================================================================================\n",
      "MLFLOW UI\n",
      "================================================================================\n",
      "\n",
      "To view all runs and compare models in MLflow UI:\n",
      "üëâ Run: mlflow ui --backend-store-uri file:../mlruns\n",
      "üëâ Open: http://localhost:5000\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ MODEL TRAINING PIPELINE COMPLETED SUCCESSFULLY\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: Compare All Models\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all runs from the experiment\n",
    "experiment = mlflow.get_experiment_by_name(\"energy-consumption-forecasting\")\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "# Display key metrics\n",
    "comparison_df = runs_df[['tags.mlflow.runName', 'metrics.rmse', 'metrics.mae', 'metrics.r2_score', 'metrics.mape']].copy()\n",
    "comparison_df.columns = ['Model', 'RMSE', 'MAE', 'R¬≤', 'MAPE (%)']\n",
    "comparison_df = comparison_df.sort_values('RMSE', ascending=True)\n",
    "\n",
    "print(\"\\nüìä Performance Comparison (sorted by RMSE):\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = comparison_df['RMSE'].idxmin()\n",
    "best_model = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_rmse = comparison_df.loc[best_model_idx, 'RMSE']\n",
    "best_r2 = comparison_df.loc[best_model_idx, 'R¬≤']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üèÜ Model: {best_model}\")\n",
    "print(f\"   RMSE: {best_rmse:.4f} kWh\")\n",
    "print(f\"   R¬≤:   {best_r2:.4f} ({best_r2*100:.1f}% variance explained)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL SELECTION INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "EXPECTED PERFORMANCE PATTERNS:\n",
    "================================\n",
    "1. LINEAR REGRESSION: \n",
    "   - Baseline model, assumes linear relationships\n",
    "   - Good if energy consumption is linear function of features\n",
    "   - Expected: Moderate performance, high interpretability\n",
    "\n",
    "2. K-NEAREST NEIGHBORS:\n",
    "   - Captures local patterns and non-linearity\n",
    "   - Sensitive to feature scaling (already handled)\n",
    "   - Expected: Good for similar time periods/conditions\n",
    "\n",
    "3. RANDOM FOREST:\n",
    "   - Handles non-linear relationships and interactions\n",
    "   - Robust to outliers, provides feature importance\n",
    "   - Expected: Strong performance, slower predictions\n",
    "\n",
    "4. XGBOOST:\n",
    "   - Often best performer for tabular data\n",
    "   - Handles complex patterns via sequential boosting\n",
    "   - Expected: Top 2 model, efficient predictions\n",
    "\n",
    "5. RULEFIT:\n",
    "   - Generates interpretable rules for stakeholders\n",
    "   - Balances accuracy with explainability\n",
    "   - Expected: Good performance + business insights\n",
    "\n",
    "6. PCA + LINEAR REGRESSION:\n",
    "   - Addresses multicollinearity issues\n",
    "   - Reduces overfitting via dimensionality reduction\n",
    "   - Expected: Better than plain LR if features correlated\n",
    "\n",
    "7. PCA + RANDOM FOREST:\n",
    "   - Tests if PCA helps tree-based models\n",
    "   - May speed up training, unlikely to improve accuracy\n",
    "   - Expected: Similar to RF, faster training\n",
    "\n",
    "8. PCA + XGBOOST:\n",
    "   - Combines dimensionality reduction with boosting\n",
    "   - May improve generalization\n",
    "   - Expected: Competitive with pure XGBoost\n",
    "\n",
    "SELECTION CRITERIA:\n",
    "===================\n",
    "‚Ä¢ PRODUCTION (Speed + Accuracy): Choose XGBoost or RF\n",
    "‚Ä¢ INTERPRETABILITY (Business): Choose RuleFit or Linear Regression  \n",
    "‚Ä¢ DEPLOYMENT (Low latency): Choose Linear Regression or KNN\n",
    "‚Ä¢ RESEARCH (Best RMSE): Choose top performer regardless of complexity\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MLFLOW UI\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTo view all runs and compare models in MLflow UI:\")\n",
    "print(\"üëâ Run: mlflow ui --backend-store-uri file:../mlruns\")\n",
    "print(\"üëâ Open: http://localhost:5000\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n‚úÖ MODEL TRAINING PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
